{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> RNN with TensorFlow</b>\n",
    "<br/>\n",
    "This notebook contains the basic RNN implementation with TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import and configuration related\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 5 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 4\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate dummy data\n",
    "def gen_data(size=1000000):\n",
    "    #generate a numpy array of size=\"size\" and with elements in the\n",
    "    # range (0,2)\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Definition of rnn_cell\n",
    "\n",
    "This is very similar to the __call__ method on Tensorflow's BasicRNNCell. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py#L95\n",
    "\"\"\"\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "    b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "\"\"\"\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\n",
    "Losses is similar to the \"sequence_loss\"\n",
    "function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "\"\"\"\n",
    "\n",
    "#logits and predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "#losses and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 for last 250 steps: 0.592390707731\n",
      "Average loss at step 200 for last 250 steps: 0.531126879454\n",
      "Average loss at step 300 for last 250 steps: 0.522759394646\n",
      "Average loss at step 400 for last 250 steps: 0.52084880352\n",
      "Average loss at step 500 for last 250 steps: 0.522119639814\n",
      "Average loss at step 600 for last 250 steps: 0.521813491881\n",
      "Average loss at step 700 for last 250 steps: 0.519929710329\n",
      "Average loss at step 800 for last 250 steps: 0.522535752654\n",
      "Average loss at step 900 for last 250 steps: 0.518634746075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xa1b95f8>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x37de128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(1,num_steps)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport basic_rnn\\ndef plot_learning_curve(num_steps, state_size=4, epochs=1):\\n    global losses, total_loss, final_state, train_step, x, y, init_state\\n    tf.reset_default_graph()\\n    g = tf.get_default_graph()\\n    losses, total_loss, final_state, train_step, x, y, init_state =         basic_rnn.setup_graph(g,\\n            basic_rnn.RNN_config(num_steps=num_steps, state_size=state_size))\\n    res = train_network(epochs, num_steps, state_size=state_size, verbose=False)\\n    plt.plot(res)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import basic_rnn\n",
    "def plot_learning_curve(num_steps, state_size=4, epochs=1):\n",
    "    global losses, total_loss, final_state, train_step, x, y, init_state\n",
    "    tf.reset_default_graph()\n",
    "    g = tf.get_default_graph()\n",
    "    losses, total_loss, final_state, train_step, x, y, init_state = \\\n",
    "        basic_rnn.setup_graph(g,\n",
    "            basic_rnn.RNN_config(num_steps=num_steps, state_size=state_size))\n",
    "    res = train_network(epochs, num_steps, state_size=state_size, verbose=False)\n",
    "    plt.plot(res)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNUM_STEPS = 1\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NUM_STEPS = 1\n",
    "\"\"\"\n",
    "#plot_learning_curve(num_steps=1, state_size=4, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNUM_STEPS = 10\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NUM_STEPS = 10\n",
    "\"\"\"\n",
    "#plot_learning_curve(num_steps=10, state_size=16, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Transforming to Tensorflow</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable softmax/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\Users\\rbaral\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n  File \"C:\\Users\\rbaral\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\rbaral\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-84408401bfce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#tf.get_variable_scope().reuse_variables()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'W'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrnn_output\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrnn_outputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m       custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m           custom_getter=custom_getter)\n\u001b[0m\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[0;32m    346\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m           validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[0;32m    637\u001b[0m                          \u001b[1;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 639\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    640\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable softmax/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"C:\\Users\\rbaral\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n  File \"C:\\Users\\rbaral\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\rbaral\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\"\"\"\n",
    "#tf.get_variable_scope().reuse_variables()\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> References </b>\n",
    "<br/>\n",
    "<ol>\n",
    "<li> Code and resource credit to <a href=\"https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\"> r2rt.com </a> </li>\n",
    "<li> <a href=\"https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html\"> Written Memories: Understanding, Deriving and Extending the LSTM, on this blog </a> </li>\n",
    "<li> <a href=\"http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\">Recurrent Neural Networks Tutorial, by Denny Britz </a> </li>\n",
    "<li> <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\"> The Unreasonable Effectiveness of Recurrent Neural Networks, by Andrej Karpathy</a> </li>\n",
    "<li> <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"> Understanding LSTM Networks, by Christopher Olah </a></li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
