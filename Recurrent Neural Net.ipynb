{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Recurrent Neural Net with TensorFlow</h4>\n",
    "<br/>\n",
    "This notebooks provides a brief overview of the recurrent neural network. We use the famous TensorFlow and its available knowledge resource to get into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, most of the source code is borrowed (and is acknowledged) from the TensorFlow website, there are some modifications made to deal with the followings:\n",
    "<ol>\n",
    "<li> decreasing learning rate schedule </li>\n",
    "<li> dropout between the LSTM layers </li>\n",
    "</ol>\n",
    "\n",
    "First we download the PTB data from  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz.\n",
    "\n",
    "As per the original resource, the dataset is already preprocessed and contains overall 10000 different words, including the end-of-sentence marker and a special symbol (\\&lt;unk&gt;) for rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Reading data </b>\n",
    "<br/>\n",
    "I have the PTB data (taken from the link mentioned above) in the ptb_data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets have the required libraries\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import inspect\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "#lets create the path for the train, validation, and test dataset (all of them are already available in the given line)\n",
    "data_path = \"ptb_data\" # you can change it to your data directory\n",
    "train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "test_path = os.path.join(data_path, \"ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the RNN processes numbers only, we need to assign some numeric identifier to each word/token in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data File: We achieve this with the following methods, which use the TensorFlow to decode the unicode text and split the data based on the newline (\\n) character. We can have any other simple methods to handle this and do not need the TensorFlow for this basic task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "reads the line and returns a list\n",
    "containing the lines from the file.\n",
    "Uses the token <eos> for the new line character \\n\n",
    "to do the splitting.\n",
    "'''\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use the above method to build the vocabulary of the words and their ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this method reads the words from a given file\n",
    "and assigns ids (starting from 0) to the words based on their frequency.\n",
    "'''\n",
    "def build_vocab(filename):\n",
    "    #get the list of sentences\n",
    "    data = read_words(filename)\n",
    "    #get the count of each tokens\n",
    "    counter = collections.Counter(data)\n",
    "    #sort the tokens by their frequency\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "    #just use the words as we do not need the frequency any more\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    #for every word get the id in the range (0, len(words))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets call the above method and see the ids of some of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word  evaluate  has id: 5385\n",
      "word  external  has id: 6030\n",
      "word  triggered  has id: 2363\n",
      "word  neuberger  has id: 9205\n",
      "word  wildlife  has id: 7224\n",
      "word  sunnyvale  has id: 5844\n",
      "word  frustration  has id: 6437\n",
      "word  index  has id: 216\n",
      "word  stake  has id: 320\n",
      "word  graduate  has id: 5683\n"
     ]
    }
   ],
   "source": [
    "word_to_id = build_vocab(train_path)\n",
    "#lets print some of the word and their id representation\n",
    "for word in list(word_to_id.keys())[:10]:\n",
    "    print(\"word \",word,\" has id:\",word_to_id[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We do the similar thing with the train, test, and the validation set. To make the task easier, we use a utility method that converts a whole file to the word-id pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = file_to_word_ids(train_path, word_to_id)\n",
    "valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "test_data = file_to_word_ids(test_path, word_to_id)\n",
    "#the vocabulary is the number of unique words in the dataset\n",
    "vocabulary = len(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> LSTM </b>\n",
    "<br/>\n",
    "We use the TensorFlow library to realize the LSTM. So, lets define some constants to be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ALERT! If you run this cell multiple times then it would generate an error \n",
    "#\"ArgumentError: argument --model: conflicting option string: ...\"\n",
    "#That might be because the same variable name is attempted to be redefined again!\n",
    "#If you need to run multiple times then restart the whole kernel\n",
    "#we define the constants in the form of TensorFlow flags\n",
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "flags.DEFINE_string(\"model\", \"small\", \"A type of model. Possible options are: small, medium, large.\")\n",
    "flags.DEFINE_string(\"data_path\", None, \"Where the training/test data is stored.\")\n",
    "flags.DEFINE_string(\"save_path\", None, \"Model output directory.\")\n",
    "flags.DEFINE_bool(\"use_fp16\", False, \"Train using 16-bit floats instead of 32bit floats\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more modular, we define additional methods that deal with the configuration, hyperparameter initialization, and so on. These are all simple initialization methods and are easy to follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class representing configuration parameters for a small model\n",
    "class SmallConfig(object):\n",
    "  \"\"\"Small config.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 20\n",
    "  hidden_size = 200\n",
    "  max_epoch = 4\n",
    "  max_max_epoch = 13\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "#class representing configuration parameters for a medium size model\n",
    "class MediumConfig(object):\n",
    "  \"\"\"Medium config.\"\"\"\n",
    "  init_scale = 0.05\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 35\n",
    "  hidden_size = 650\n",
    "  max_epoch = 6\n",
    "  max_max_epoch = 39\n",
    "  keep_prob = 0.5\n",
    "  lr_decay = 0.8\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "#class representing configuration parameters for a large size model\n",
    "class LargeConfig(object):\n",
    "  \"\"\"Large config.\"\"\"\n",
    "  init_scale = 0.04\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 10\n",
    "  num_layers = 2\n",
    "  num_steps = 35\n",
    "  hidden_size = 1500\n",
    "  max_epoch = 14\n",
    "  max_max_epoch = 55\n",
    "  keep_prob = 0.35\n",
    "  lr_decay = 1 / 1.15\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "#class representing configuration parameters for a test model\n",
    "class TestConfig(object):\n",
    "  \"\"\"Tiny config, for testing.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 1\n",
    "  num_layers = 1\n",
    "  num_steps = 2\n",
    "  hidden_size = 2\n",
    "  max_epoch = 1\n",
    "  max_max_epoch = 1\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the desired configuration based on the type of model, we use the small model by default as\n",
    "#defined in our FLAG variable\n",
    "def get_config():\n",
    "    if FLAGS.model == \"small\":\n",
    "        return SmallConfig()\n",
    "    elif FLAGS.model == \"medium\":\n",
    "        return MediumConfig()\n",
    "    elif FLAGS.model == \"large\":\n",
    "        return LargeConfig()\n",
    "    elif FLAGS.model == \"test\":\n",
    "        return TestConfig()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model: %s\", FLAGS.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets call the above methods to get the configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "eval_config = get_config() #configuration for evaluation\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#based on the flag configured, we use the data type\n",
    "def data_type():\n",
    "  return tf.float16 if FLAGS.use_fp16 else tf.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorFlow first defines the problem in terms of a graph. Lets define a graph to realize the problem and feed the variables and parameters into the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a method that prepares the PTB data in the form of tensors that will be processed by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ptb_producer(raw_data, batch_size, num_steps, name=None):\n",
    "  \"\"\"Iterate on the raw PTB data.\n",
    "  This chunks up raw_data into batches of examples and returns Tensors that\n",
    "  are drawn from these batches.\n",
    "  Args:\n",
    "    raw_data: the raw ptb data (train data, test data or validation data).\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "    name: the name of this operation (optional).\n",
    "  Returns:\n",
    "    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "    of the tuple is the same data time-shifted to the right by one.\n",
    "  Raises:\n",
    "    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n",
    "                      [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    \n",
    "    #lets make sure that we have a positive value for epoch_size\n",
    "    assertion = tf.assert_positive(\n",
    "        epoch_size,\n",
    "        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    \n",
    "    #lets add the condition/assertion to the context of this session\n",
    "    with tf.control_dependencies([assertion]):\n",
    "      epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "    #lets produce an integer in the range (0, epoch_size-1) in the queue    \n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    #before we move ahead, lets see some example to make sure we get the idea behind the slicing\n",
    "    # 'input' is [[[1, 1, 1], [2, 2, 2]],\n",
    "    #             [[3, 3, 3], [4, 4, 4]],\n",
    "    #             [[5, 5, 5], [6, 6, 6]]]\n",
    "    #tf.slice(input, [1, 0, 0], [1, 1, 3]) ==> [[[3, 3, 3]]]\n",
    "    #==>input is the data to be sliced\n",
    "    #==> [1, 0 ,0] means the begining point from where the slicing begins. It is the second list's (the first element in [1,0,0]), \n",
    "    #==>         first element's (the second element in [1,0,0]), first element (the third element in [1,0,0]), so this points to the\n",
    "    #==>         sublist [3,3,3] (make sure you get this!)\n",
    "    #==> [1,1,3] means we consider one list (the first element of [1,1,3]) from the begin, and we will get 1*3 sized tensor from there,\n",
    "    #           which gives [3,3,3] as the desired result\n",
    "    #Still confused? Lets see another example:\n",
    "    #tf.slice(input, [1, 0, 0], [1, 2, 3]) ==> [[[3, 3, 3],[4, 4, 4]]]\n",
    "    #==> here also the begining point is [1,0,0] which is same as the previous example. Now we are considering one list (the first element of [1,2,3])\n",
    "    #   this means we are considering only the list [[3,3,3] [4,4,4]]. From this, we extract a 2*3 tensor from each of the list, which gives us [[3,3,3][4,4,4]]\n",
    "    #   I hope its a bit more clear than the first one. We still have one more to go:)\n",
    "    #tf.slice(input, [1, 0, 0], [2, 1, 3]) ==> [[[3, 3, 3]],[[5, 5, 5]]]\n",
    "    #==> Again the begining point is same. Now we have the size parameter as [2,1,3], which means we are considering two lists (the first element of [2,1,3])\n",
    "    #   from the begining part. Our two lists from the begining are: [[3,3,3][4,4,4]] and [[5,5,5][6,6,6]]. Now we are extracting a 1*3 tensor from\n",
    "    #   each of the list we are considering. As we are considering two lists, we will get the first 1*3 tensor from each of the list. This gives us\n",
    "    #   [3,3,3] from the first one and [5,5,5] from the second one\n",
    "    #lets extract a slice from the data tensor, the begin and end of the slice is given by\n",
    "    #..the other two parameters\n",
    "    x = tf.strided_slice(data, [0, i * num_steps],\n",
    "                         [batch_size, (i + 1) * num_steps])\n",
    "    #reshape the slice\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    #create another slice from the data\n",
    "    y = tf.strided_slice(data, [0, i * num_steps + 1],\n",
    "                         [batch_size, (i + 1) * num_steps + 1])\n",
    "    #reshape the slice\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#a wrapper of PTB to make it easier to access all the related parameters\n",
    "class PTBInput(object):\n",
    "  \"\"\"The input data.\"\"\"\n",
    "\n",
    "  def __init__(self, config, data, name=None):\n",
    "    self.batch_size = batch_size = config.batch_size\n",
    "    self.num_steps = num_steps = config.num_steps\n",
    "    self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "    self.input_data, self.targets = ptb_producer(\n",
    "        data, batch_size, num_steps, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> LSTM TensorFlow Graph </b>\n",
    "<br />\n",
    "We create the LSTM network within another wrapper and later use it to execute the graph.\n",
    "So, lets define the LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "  \"\"\"The PTB model.\"\"\"\n",
    "\n",
    "  def __init__(self, is_training, config, input_):\n",
    "    self._input = input_\n",
    "\n",
    "    batch_size = input_.batch_size\n",
    "    num_steps = input_.num_steps\n",
    "    size = config.hidden_size\n",
    "    vocab_size = config.vocab_size\n",
    "\n",
    "    # Slightly better results can be obtained with forget gate biases\n",
    "    # initialized to 1 but the hyperparameters of the model would need to be\n",
    "    # different than reported in the paper.\n",
    "    def lstm_cell():\n",
    "      # With the latest TensorFlow source code (as of Mar 27, 2017),\n",
    "      # the BasicLSTMCell will need a reuse parameter which is unfortunately not\n",
    "      # defined in TensorFlow 1.0. To maintain backwards compatibility, we add\n",
    "      # an argument check here:\n",
    "      if 'reuse' in inspect.getargspec(\n",
    "          tf.contrib.rnn.BasicLSTMCell.__init__).args:\n",
    "        return tf.contrib.rnn.BasicLSTMCell(\n",
    "            size, forget_bias=0.0, state_is_tuple=True,\n",
    "            reuse=tf.get_variable_scope().reuse)\n",
    "      else:\n",
    "        return tf.contrib.rnn.BasicLSTMCell(\n",
    "            size, forget_bias=0.0, state_is_tuple=True)\n",
    "    attn_cell = lstm_cell\n",
    "    if is_training and config.keep_prob < 1:\n",
    "        def attn_cell():\n",
    "            return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=config.keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [attn_cell() for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "\n",
    "    self._initial_state = cell.zero_state(batch_size, data_type())\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "      embedding = tf.get_variable(\n",
    "          \"embedding\", [vocab_size, size], dtype=data_type())\n",
    "      inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
    "\n",
    "    if is_training and config.keep_prob < 1:\n",
    "      inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "    # Simplified version of models/tutorials/rnn/rnn.py's rnn().\n",
    "    # This builds an unrolled LSTM for tutorial purposes only.\n",
    "    # In general, use the rnn() or state_saving_rnn() from rnn.py.\n",
    "    #\n",
    "    # The alternative version of the code below is:\n",
    "    #\n",
    "    # inputs = tf.unstack(inputs, num=num_steps, axis=1)\n",
    "    # outputs, state = tf.contrib.rnn.static_rnn(\n",
    "    #     cell, inputs, initial_state=self._initial_state)\n",
    "    outputs = []\n",
    "    state = self._initial_state\n",
    "    with tf.variable_scope(\"RNN\"):\n",
    "      for time_step in range(num_steps):\n",
    "        if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "        (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
    "        outputs.append(cell_output)\n",
    "\n",
    "    output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])\n",
    "    softmax_w = tf.get_variable(\n",
    "        \"softmax_w\", [size, vocab_size], dtype=data_type())\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=data_type())\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [tf.reshape(input_.targets, [-1])],\n",
    "        [tf.ones([batch_size * num_steps], dtype=data_type())])\n",
    "    self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "    self._final_state = state\n",
    "\n",
    "    if not is_training:\n",
    "      return\n",
    "\n",
    "    self._lr = tf.Variable(0.0, trainable=False)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n",
    "                                      config.max_grad_norm)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "    self._train_op = optimizer.apply_gradients(\n",
    "        zip(grads, tvars),\n",
    "        global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "    self._new_lr = tf.placeholder(\n",
    "        tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "    self._lr_update = tf.assign(self._lr, self._new_lr)\n",
    "\n",
    "  def assign_lr(self, session, lr_value):\n",
    "    session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
    "\n",
    "  @property\n",
    "  def input(self):\n",
    "    return self._input\n",
    "\n",
    "  @property\n",
    "  def initial_state(self):\n",
    "    return self._initial_state\n",
    "\n",
    "  @property\n",
    "  def cost(self):\n",
    "    return self._cost\n",
    "\n",
    "  @property\n",
    "  def final_state(self):\n",
    "    return self._final_state\n",
    "\n",
    "  @property\n",
    "  def lr(self):\n",
    "    return self._lr\n",
    "\n",
    "  @property\n",
    "  def train_op(self):\n",
    "    return self._train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, eval_op=None, verbose=False):\n",
    "  \"\"\"Runs the model on the given data.\"\"\"\n",
    "  start_time = time.time()\n",
    "  costs = 0.0\n",
    "  iters = 0\n",
    "  state = session.run(model.initial_state)\n",
    "\n",
    "  fetches = {\n",
    "      \"cost\": model.cost,\n",
    "      \"final_state\": model.final_state,\n",
    "  }\n",
    "  if eval_op is not None:\n",
    "    fetches[\"eval_op\"] = eval_op\n",
    "\n",
    "  for step in range(model.input.epoch_size):\n",
    "    feed_dict = {}\n",
    "    for i, (c, h) in enumerate(model.initial_state):\n",
    "      feed_dict[c] = state[i].c\n",
    "      feed_dict[h] = state[i].h\n",
    "\n",
    "    vals = session.run(fetches, feed_dict)\n",
    "    cost = vals[\"cost\"]\n",
    "    state = vals[\"final_state\"]\n",
    "\n",
    "    costs += cost\n",
    "    iters += model.input.num_steps\n",
    "\n",
    "    if verbose and step % (model.input.epoch_size // 10) == 10:\n",
    "      print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "            (step * 1.0 / model.input.epoch_size, np.exp(costs / iters),\n",
    "             iters * model.input.batch_size / (time.time() - start_time)))\n",
    "\n",
    "  return np.exp(costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "0.004 perplexity: 6289.963 speed: 1260 wps\n",
      "0.104 perplexity: 856.345 speed: 1390 wps\n",
      "0.204 perplexity: 630.369 speed: 1394 wps\n",
      "0.304 perplexity: 508.410 speed: 1392 wps\n",
      "0.404 perplexity: 438.189 speed: 1390 wps\n",
      "0.504 perplexity: 392.381 speed: 1388 wps\n",
      "0.604 perplexity: 353.348 speed: 1379 wps\n",
      "0.703 perplexity: 326.705 speed: 1367 wps\n",
      "0.803 perplexity: 305.498 speed: 1367 wps\n",
      "0.903 perplexity: 286.074 speed: 1371 wps\n",
      "Epoch: 1 Train_Perplexity: 271.529\n",
      "Epoch: 1 Valid Perplexity: 182.257\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "0.004 perplexity: 214.086 speed: 1375 wps\n",
      "0.104 perplexity: 152.839 speed: 1401 wps\n",
      "0.204 perplexity: 159.960 speed: 1400 wps\n",
      "0.304 perplexity: 154.776 speed: 1403 wps\n",
      "0.404 perplexity: 151.870 speed: 1404 wps\n",
      "0.504 perplexity: 149.373 speed: 1405 wps\n",
      "0.604 perplexity: 144.693 speed: 1406 wps\n",
      "0.703 perplexity: 142.510 speed: 1406 wps\n",
      "0.803 perplexity: 140.507 speed: 1406 wps\n",
      "0.903 perplexity: 136.877 speed: 1406 wps\n",
      "Epoch: 2 Train_Perplexity: 134.805\n",
      "Epoch: 2 Valid Perplexity: 144.544\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "0.004 perplexity: 147.327 speed: 1406 wps\n",
      "0.104 perplexity: 106.505 speed: 1404 wps\n",
      "0.204 perplexity: 115.312 speed: 1404 wps\n",
      "0.304 perplexity: 112.474 speed: 1405 wps\n",
      "0.404 perplexity: 111.566 speed: 1406 wps\n",
      "0.504 perplexity: 110.643 speed: 1406 wps\n",
      "0.604 perplexity: 107.974 speed: 1406 wps\n",
      "0.703 perplexity: 107.266 speed: 1406 wps\n",
      "0.803 perplexity: 106.651 speed: 1406 wps\n",
      "0.903 perplexity: 104.429 speed: 1407 wps\n",
      "Epoch: 3 Train_Perplexity: 103.437\n",
      "Epoch: 3 Valid Perplexity: 132.844\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "0.004 perplexity: 116.241 speed: 1415 wps\n",
      "0.104 perplexity: 85.993 speed: 1405 wps\n",
      "0.204 perplexity: 94.153 speed: 1405 wps\n",
      "0.304 perplexity: 92.074 speed: 1406 wps\n",
      "0.404 perplexity: 91.675 speed: 1407 wps\n",
      "0.504 perplexity: 91.279 speed: 1407 wps\n",
      "0.604 perplexity: 89.393 speed: 1407 wps\n",
      "0.703 perplexity: 89.204 speed: 1407 wps\n",
      "0.803 perplexity: 89.009 speed: 1407 wps\n",
      "0.903 perplexity: 87.360 speed: 1407 wps\n",
      "Epoch: 4 Train_Perplexity: 86.777\n",
      "Epoch: 4 Valid Perplexity: 128.234\n",
      "Epoch: 5 Learning rate: 0.500\n",
      "0.004 perplexity: 99.945 speed: 1408 wps\n",
      "0.104 perplexity: 71.685 speed: 1408 wps\n",
      "0.204 perplexity: 77.624 speed: 1408 wps\n",
      "0.304 perplexity: 74.889 speed: 1408 wps\n",
      "0.404 perplexity: 73.749 speed: 1407 wps\n",
      "0.504 perplexity: 72.798 speed: 1407 wps\n",
      "0.604 perplexity: 70.639 speed: 1407 wps\n",
      "0.703 perplexity: 69.881 speed: 1407 wps\n",
      "0.803 perplexity: 69.128 speed: 1407 wps\n",
      "0.903 perplexity: 67.220 speed: 1407 wps\n",
      "Epoch: 5 Train_Perplexity: 66.168\n",
      "Epoch: 5 Valid Perplexity: 119.119\n",
      "Epoch: 6 Learning rate: 0.250\n",
      "0.004 perplexity: 81.597 speed: 1410 wps\n",
      "0.104 perplexity: 59.268 speed: 1406 wps\n",
      "0.204 perplexity: 64.225 speed: 1407 wps\n",
      "0.304 perplexity: 61.848 speed: 1406 wps\n",
      "0.404 perplexity: 60.834 speed: 1406 wps\n",
      "0.504 perplexity: 60.012 speed: 1406 wps\n",
      "0.604 perplexity: 58.107 speed: 1405 wps\n",
      "0.703 perplexity: 57.358 speed: 1405 wps\n",
      "0.803 perplexity: 56.580 speed: 1405 wps\n",
      "0.903 perplexity: 54.858 speed: 1405 wps\n",
      "Epoch: 6 Train_Perplexity: 53.840\n",
      "Epoch: 6 Valid Perplexity: 118.496\n",
      "Epoch: 7 Learning rate: 0.125\n",
      "0.004 perplexity: 72.186 speed: 1413 wps\n",
      "0.104 perplexity: 52.407 speed: 1407 wps\n",
      "0.204 perplexity: 57.055 speed: 1408 wps\n",
      "0.304 perplexity: 54.908 speed: 1407 wps\n",
      "0.404 perplexity: 53.977 speed: 1407 wps\n",
      "0.504 perplexity: 53.214 speed: 1406 wps\n",
      "0.604 perplexity: 51.497 speed: 1406 wps\n",
      "0.703 perplexity: 50.797 speed: 1406 wps\n",
      "0.803 perplexity: 50.030 speed: 1406 wps\n",
      "0.903 perplexity: 48.430 speed: 1406 wps\n",
      "Epoch: 7 Train_Perplexity: 47.465\n",
      "Epoch: 7 Valid Perplexity: 119.599\n",
      "Epoch: 8 Learning rate: 0.062\n",
      "0.004 perplexity: 67.684 speed: 1410 wps\n",
      "0.104 perplexity: 49.032 speed: 1406 wps\n",
      "0.204 perplexity: 53.483 speed: 1405 wps\n",
      "0.304 perplexity: 51.430 speed: 1406 wps\n",
      "0.404 perplexity: 50.544 speed: 1406 wps\n",
      "0.504 perplexity: 49.803 speed: 1405 wps\n",
      "0.604 perplexity: 48.170 speed: 1406 wps\n",
      "0.703 perplexity: 47.495 speed: 1406 wps\n",
      "0.803 perplexity: 46.736 speed: 1406 wps\n",
      "0.903 perplexity: 45.198 speed: 1406 wps\n",
      "Epoch: 8 Train_Perplexity: 44.265\n",
      "Epoch: 8 Valid Perplexity: 120.253\n",
      "Epoch: 9 Learning rate: 0.031\n",
      "0.004 perplexity: 65.429 speed: 1398 wps\n",
      "0.104 perplexity: 47.267 speed: 1366 wps\n",
      "0.204 perplexity: 51.598 speed: 1339 wps\n",
      "0.304 perplexity: 49.601 speed: 1339 wps\n",
      "0.404 perplexity: 48.742 speed: 1355 wps\n",
      "0.504 perplexity: 48.026 speed: 1365 wps\n",
      "0.604 perplexity: 46.439 speed: 1371 wps\n",
      "0.703 perplexity: 45.770 speed: 1376 wps\n",
      "0.803 perplexity: 45.011 speed: 1380 wps\n",
      "0.903 perplexity: 43.509 speed: 1383 wps\n",
      "Epoch: 9 Train_Perplexity: 42.592\n",
      "Epoch: 9 Valid Perplexity: 120.322\n",
      "Epoch: 10 Learning rate: 0.016\n",
      "0.004 perplexity: 64.135 speed: 1419 wps\n",
      "0.104 perplexity: 46.296 speed: 1401 wps\n",
      "0.204 perplexity: 50.568 speed: 1400 wps\n",
      "0.304 perplexity: 48.602 speed: 1399 wps\n",
      "0.404 perplexity: 47.751 speed: 1397 wps\n",
      "0.504 perplexity: 47.052 speed: 1393 wps\n",
      "0.604 perplexity: 45.495 speed: 1394 wps\n",
      "0.703 perplexity: 44.830 speed: 1396 wps\n",
      "0.803 perplexity: 44.069 speed: 1396 wps\n",
      "0.903 perplexity: 42.586 speed: 1397 wps\n",
      "Epoch: 10 Train_Perplexity: 41.680\n",
      "Epoch: 10 Valid Perplexity: 120.164\n",
      "Epoch: 11 Learning rate: 0.008\n",
      "0.004 perplexity: 63.321 speed: 1367 wps\n",
      "0.104 perplexity: 45.758 speed: 1395 wps\n",
      "0.204 perplexity: 50.008 speed: 1398 wps\n",
      "0.304 perplexity: 48.049 speed: 1396 wps\n",
      "0.404 perplexity: 47.200 speed: 1398 wps\n",
      "0.504 perplexity: 46.504 speed: 1399 wps\n",
      "0.604 perplexity: 44.964 speed: 1400 wps\n",
      "0.703 perplexity: 44.305 speed: 1400 wps\n",
      "0.803 perplexity: 43.546 speed: 1401 wps\n",
      "0.903 perplexity: 42.074 speed: 1401 wps\n",
      "Epoch: 11 Train_Perplexity: 41.174\n",
      "Epoch: 11 Valid Perplexity: 119.883\n",
      "Epoch: 12 Learning rate: 0.004\n",
      "0.004 perplexity: 62.827 speed: 1369 wps\n",
      "0.104 perplexity: 45.446 speed: 1396 wps\n",
      "0.204 perplexity: 49.692 speed: 1393 wps\n",
      "0.304 perplexity: 47.741 speed: 1394 wps\n",
      "0.404 perplexity: 46.898 speed: 1395 wps\n",
      "0.504 perplexity: 46.201 speed: 1392 wps\n",
      "0.604 perplexity: 44.672 speed: 1393 wps\n",
      "0.703 perplexity: 44.015 speed: 1393 wps\n",
      "0.803 perplexity: 43.260 speed: 1395 wps\n",
      "0.903 perplexity: 41.796 speed: 1398 wps\n",
      "Epoch: 12 Train_Perplexity: 40.899\n",
      "Epoch: 12 Valid Perplexity: 119.602\n",
      "Epoch: 13 Learning rate: 0.002\n",
      "0.004 perplexity: 62.534 speed: 1381 wps\n",
      "0.104 perplexity: 45.259 speed: 1361 wps\n",
      "0.204 perplexity: 49.502 speed: 1357 wps\n",
      "0.304 perplexity: 47.564 speed: 1366 wps\n",
      "0.404 perplexity: 46.730 speed: 1370 wps\n",
      "0.504 perplexity: 46.034 speed: 1374 wps\n",
      "0.604 perplexity: 44.511 speed: 1377 wps\n",
      "0.703 perplexity: 43.857 speed: 1381 wps\n",
      "0.803 perplexity: 43.105 speed: 1383 wps\n",
      "0.903 perplexity: 41.646 speed: 1385 wps\n",
      "Epoch: 13 Train_Perplexity: 40.751\n",
      "Epoch: 13 Valid Perplexity: 119.419\n",
      "Test Perplexity: 115.416\n"
     ]
    }
   ],
   "source": [
    "#the default graph\n",
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale,\n",
    "                                                config.init_scale)\n",
    "    #create a graph variable representing the training set\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        train_input = PTBInput(config=config, data=train_data, name=\"TrainInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "            m = PTBModel(is_training=True, config=config, input_=train_input)\n",
    "        tf.summary.scalar(\"Training_Loss\", m._cost)\n",
    "        tf.summary.scalar(\"Learning_Rate\", m._lr)\n",
    "        \n",
    "    #now create the graph variable representing the validation set\n",
    "    with tf.name_scope(\"Valid\"):\n",
    "        valid_input = PTBInput(config=config, data=valid_data, name=\"ValidInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mvalid = PTBModel(is_training=False, config=config, input_=valid_input)\n",
    "        tf.summary.scalar(\"Validation_Loss\", mvalid._cost)\n",
    "\n",
    "    #create the variable representing the test set\n",
    "    with tf.name_scope(\"Test\"):\n",
    "        test_input = PTBInput(config=eval_config, data=test_data, name=\"TestInput\")\n",
    "        with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "            mtest = PTBModel(is_training=False, config=eval_config,\n",
    "                         input_=test_input)\n",
    "    #the supervisor class is a small wrapper that takes care of common needs of TensorFlow training program,\n",
    "    # for instance, handling program crashes, notifying of raised exceptions, and so on (ref:https://www.tensorflow.org/api_docs/python/tf/train/Supervisor)\n",
    "    sv = tf.train.Supervisor(logdir=FLAGS.save_path)\n",
    "    with sv.managed_session() as session:\n",
    "        for i in range(config.max_max_epoch):\n",
    "            lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)\n",
    "            m.assign_lr(session, config.learning_rate * lr_decay)\n",
    "\n",
    "            print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "            train_perplexity = run_epoch(session, m, eval_op=m.train_op,\n",
    "                                     verbose=True)\n",
    "            print(\"Epoch: %d Train_Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "            valid_perplexity = run_epoch(session, mvalid)\n",
    "            print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "        test_perplexity = run_epoch(session, mtest)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "\n",
    "        if FLAGS.save_path:\n",
    "            print(\"Saving model to %s.\" % FLAGS.save_path)\n",
    "            sv.saver.save(session, FLAGS.save_path, global_step=sv.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Model </b>\n",
    "<br />\n",
    "The model consists of LSTM cells. Each LSTM processes one word at a time and finds the probabilities of the likely values for the next word in the sentence.\n",
    "It is better to use mini-batches to make the program run with feasible computational cost.\n",
    "<!-- $c = \\sqrt{a^2 + b^2}$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1351\u001b[1;33m       \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1352\u001b[0m       \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m    799\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    377\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types)\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'LSTMStateTuple'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-01f17b7268f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mlstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Initial state of the LSTM memory.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprobabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mzeros\u001b[1;34m(shape, dtype, name)\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1354\u001b[1;33m       \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m       \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzero\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m   \u001b[1;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[0;32m    635\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    108\u001b[0m                                          as_ref=False):\n\u001b[0;32m    109\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m---> 99\u001b[1;33m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    100\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32mC:\\Users\\rbara012.AD.004\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m       \u001b[0m_AssertCompatible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m       \u001b[0mnparray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m       \u001b[1;31m# check to them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m       \u001b[1;31m# We need to pass in quantized values as tuples, so don't apply the shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#lets declare the number of hidden layers we want to use\n",
    "lstm_size = 3 # we can also use the one in the config\n",
    "batch_size = eval_config.batch_size\n",
    "'''\n",
    "The core of the model consists of an LSTM cell that processes one word at a time \n",
    "and computes probabilities of the possible values for the next word in the sentence. \n",
    "The memory state of the network is initialized with a vector of zeros and gets updated \n",
    "after reading each word. \n",
    "For computational reasons, we will process data in mini-batches of size batch_size.\n",
    "'''\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "# Initial state of the LSTM memory.\n",
    "state = tf.zeros([batch_size, lstm.state_size])\n",
    "probabilities = []\n",
    "loss = 0.0\n",
    "for current_batch_of_words in train_data:\n",
    "    # The value of state is updated after processing each batch of words.\n",
    "    output, state = lstm(current_batch_of_words, state)\n",
    "\n",
    "    # The LSTM output can be used to make next word predictions\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    probabilities.append(tf.nn.softmax(logits))\n",
    "    loss+= loss_function(probabilities, target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Multiple LSTMs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> References</h5>\n",
    "<br />\n",
    "    <ol>\n",
    "    <li> <a href=\"https://www.tensorflow.org/tutorials/recurrent\"> TensorFlow RNN</a> </li>\n",
    "    <li> <a href=\"https://catalog.ldc.upenn.edu/ldc99t42\">Penn Tree Bank (PTB) </a> </li>\n",
    "    <li> Paper from Zaremba et al. (Recurrent Neural Network Regularization https://arxiv.org/abs/1409.2329) </li>\n",
    "    <li> PTB data  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
